{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Character Level LSTM.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/GingerBoredMan/Character_Level_Text_Generation/blob/master/Character_Level_LSTM.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o0FjVS1eNP9Z",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "from torch import nn\n",
        "import torch.nn.functional as F"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q1dmyqeoNmWq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "with open('anna.txt', 'r') as f:\n",
        "  text = f.read()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WSaUidGdOd8P",
        "colab_type": "code",
        "outputId": "b8d78da0-a4ae-42e4-ee21-85212e832356",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "text[:100]"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Chapter 1\\n\\n\\nHappy families are all alike; every unhappy family is unhappy in its own\\nway.\\n\\nEverythin'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fBJVuzDVOfuJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "chars = tuple(set(text))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I7zQRc5DO2HJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "int2char = dict(enumerate(chars))\n",
        "char2int = {ch:ii for ii, ch in int2char.items()}\n",
        "\n",
        "encoded = np.array([char2int[ch] for ch in text])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3lKbPbFpPYxM",
        "colab_type": "code",
        "outputId": "9265962e-e57d-476d-e7a2-d7be7827ba76",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 119
        }
      },
      "source": [
        "encoded [:100]"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([68, 38, 32,  8, 72, 12, 39, 82, 11, 54, 54, 54, 28, 32,  8,  8, 75,\n",
              "       82, 69, 32, 58, 36,  1, 36, 12, 79, 82, 32, 39, 12, 82, 32,  1,  1,\n",
              "       82, 32,  1, 36, 35, 12, 25, 82, 12, 76, 12, 39, 75, 82, 21, 77, 38,\n",
              "       32,  8,  8, 75, 82, 69, 32, 58, 36,  1, 75, 82, 36, 79, 82, 21, 77,\n",
              "       38, 32,  8,  8, 75, 82, 36, 77, 82, 36, 72, 79, 82, 31, 24, 77, 54,\n",
              "       24, 32, 75, 60, 54, 54,  3, 76, 12, 39, 75, 72, 38, 36, 77])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GQHMLgM1Ph6G",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def one_hot_encode(arr, n_labels):\n",
        "    \n",
        "  \n",
        "    one_hot = np.zeros((np.multiply(*arr.shape), n_labels), dtype=np.float32)\n",
        "    \n",
        "   \n",
        "    one_hot[np.arange(one_hot.shape[0]), arr.flatten()] = 1.\n",
        "    \n",
        "   \n",
        "    one_hot = one_hot.reshape((*arr.shape, n_labels))\n",
        "    \n",
        "    return one_hot"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O_xu78MaQKBu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_batches(arr, batch_size, seq_length):\n",
        " \n",
        "    \n",
        "    batch_size_total = batch_size * seq_length\n",
        "    \n",
        "    n_batches = len(arr)//batch_size_total\n",
        "    \n",
        "    arr = arr[:n_batches*batch_size_total]\n",
        "    \n",
        "    arr = arr.reshape((batch_size,-1))\n",
        "    \n",
        "    for n in range(0, arr.shape[1], seq_length):\n",
        "       \n",
        "        x = arr[:, n:n+seq_length]\n",
        "        y = np.zeros_like(x)\n",
        "        try:\n",
        "            y[:, :-1], y[:, -1] = x[:, 1:], arr[:, n+seq_length]\n",
        "        except IndexError:\n",
        "            y[:, :-1], y[:, -1] = x[:, 1:], arr[:, 0]\n",
        "        yield x, y"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8PT1PC-jX6Cg",
        "colab_type": "code",
        "outputId": "170d4a64-8e58-47ba-e68a-c8e549ac4d32",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "train_on_gpu = torch.cuda.is_available()\n",
        "if(train_on_gpu):\n",
        "    print('Training on GPU!')\n",
        "else: \n",
        "    print('No GPU available, training on CPU; consider making n_epochs very small.')"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Training on GPU!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aIAsgr83SZCF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class model(nn.Module):\n",
        "    \n",
        "    def __init__(self, tokens, n_hidden=256, n_layers=2,\n",
        "                               drop_prob=0.5, lr=0.001):\n",
        "        super().__init__()\n",
        "        self.drop_prob = drop_prob\n",
        "        self.n_layers = n_layers\n",
        "        self.n_hidden = n_hidden\n",
        "        self.lr = lr\n",
        "    \n",
        "        self.chars = tokens\n",
        "        self.int2char = dict(enumerate(self.chars))\n",
        "        self.char2int = {ch: ii for ii, ch in self.int2char.items()}\n",
        "    \n",
        "        self.lstm = nn.LSTM(input_size = len(self.chars), hidden_size = self.n_hidden, num_layers = self.n_layers,\n",
        "                        dropout = self.drop_prob, batch_first = True)\n",
        "    \n",
        "        self.dropout = nn.Dropout(drop_prob)\n",
        "    \n",
        "        self.fc = nn.Linear(n_hidden, len(self.chars))\n",
        "    \n",
        "    def forward(self, x, hidden):\n",
        "                \n",
        "        r_output, hidden = self.lstm(x, hidden)\n",
        "        \n",
        "        out = self.dropout(r_output)\n",
        "        out = out.contiguous().view(-1, self.n_hidden)\n",
        "        \n",
        "        out = self.fc(out)\n",
        "        \n",
        "\n",
        "        return out, hidden \n",
        "      \n",
        "    \n",
        "    def init_hidden(self, batch_size):\n",
        "        weight = next(self.parameters()).data\n",
        "        \n",
        "        if (train_on_gpu):\n",
        "            hidden = (weight.new(self.n_layers, batch_size, self.n_hidden).zero_().cuda(),\n",
        "                  weight.new(self.n_layers, batch_size, self.n_hidden).zero_().cuda())\n",
        "        else:\n",
        "            hidden = (weight.new(self.n_layers, batch_size, self.n_hidden).zero_(),\n",
        "                      weight.new(self.n_layers, batch_size, self.n_hidden).zero_())\n",
        "        \n",
        "        return hidden\n",
        "    "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ARQ8rwnBXMfn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def train(net, data, epochs=10, batch_size=10, seq_length=50, lr=0.001, clip=5, val_frac=0.1, print_every=10):\n",
        "    \n",
        "    net.train()\n",
        "    \n",
        "    opt = torch.optim.Adam(net.parameters(), lr=lr)\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    \n",
        "    val_idx = int(len(data)*(1-val_frac))\n",
        "    data, val_data = data[:val_idx], data[val_idx:]\n",
        "    \n",
        "    if(train_on_gpu):\n",
        "        net.cuda()\n",
        "    \n",
        "    counter = 0\n",
        "    n_chars = len(net.chars)\n",
        "    for e in range(epochs):\n",
        "        h = net.init_hidden(batch_size)\n",
        "        \n",
        "        for x, y in get_batches(data, batch_size, seq_length):\n",
        "            counter += 1\n",
        "            \n",
        "            x = one_hot_encode(x, n_chars)\n",
        "            inputs, targets = torch.from_numpy(x), torch.from_numpy(y)\n",
        "            \n",
        "            if(train_on_gpu):\n",
        "                inputs, targets = inputs.cuda(), targets.cuda()\n",
        "\n",
        "            h = tuple([each.data for each in h])\n",
        "\n",
        "            net.zero_grad()\n",
        "            \n",
        "            output, h = net(inputs, h)\n",
        "            \n",
        "            loss = criterion(output, targets.view(batch_size*seq_length).long())\n",
        "            loss.backward()\n",
        "            nn.utils.clip_grad_norm_(net.parameters(), clip)\n",
        "            opt.step()\n",
        "            \n",
        "            if counter % print_every == 0:\n",
        "                val_h = net.init_hidden(batch_size)\n",
        "                val_losses = []\n",
        "                net.eval()\n",
        "                for x, y in get_batches(val_data, batch_size, seq_length):\n",
        "                    x = one_hot_encode(x, n_chars)\n",
        "                    x, y = torch.from_numpy(x), torch.from_numpy(y)\n",
        "                    \n",
        "                    val_h = tuple([each.data for each in val_h])\n",
        "                    \n",
        "                    inputs, targets = x, y\n",
        "                    if(train_on_gpu):\n",
        "                        inputs, targets = inputs.cuda(), targets.cuda()\n",
        "\n",
        "                    output, val_h = net(inputs, val_h)\n",
        "                    val_loss = criterion(output, targets.view(batch_size*seq_length).long())\n",
        "                \n",
        "                    val_losses.append(val_loss.item())\n",
        "                \n",
        "                net.train() \n",
        "                \n",
        "                print(\"Epoch: {}/{}...\".format(e+1, epochs),\n",
        "                      \"Step: {}...\".format(counter),\n",
        "                      \"Loss: {:.4f}...\".format(loss.item()),\n",
        "                      \"Val Loss: {:.4f}\".format(np.mean(val_losses)))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3sXUDlqOZUeE",
        "colab_type": "code",
        "outputId": "99659667-29e9-48a8-be77-c03b1b7f6e69",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        }
      },
      "source": [
        "n_hidden=512\n",
        "n_layers=2\n",
        "\n",
        "net = model(chars, n_hidden, n_layers)\n",
        "print(net)"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "model(\n",
            "  (lstm): LSTM(83, 512, num_layers=2, batch_first=True, dropout=0.5)\n",
            "  (dropout): Dropout(p=0.5)\n",
            "  (fc): Linear(in_features=512, out_features=83, bias=True)\n",
            ")\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HHr1t5wocX-H",
        "colab_type": "code",
        "outputId": "b09956ba-8590-4e87-e3bf-1ff15f056e82",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 4743
        }
      },
      "source": [
        "batch_size = 128\n",
        "seq_length = 100\n",
        "n_epochs = 20\n",
        "\n",
        "train(net, encoded, epochs=n_epochs, batch_size=batch_size, seq_length=seq_length, lr=0.001, print_every=10)"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch: 1/20... Step: 10... Loss: 3.2383... Val Loss: 3.1914\n",
            "Epoch: 1/20... Step: 20... Loss: 3.1430... Val Loss: 3.1335\n",
            "Epoch: 1/20... Step: 30... Loss: 3.1365... Val Loss: 3.1213\n",
            "Epoch: 1/20... Step: 40... Loss: 3.1113... Val Loss: 3.1194\n",
            "Epoch: 1/20... Step: 50... Loss: 3.1385... Val Loss: 3.1167\n",
            "Epoch: 1/20... Step: 60... Loss: 3.1152... Val Loss: 3.1140\n",
            "Epoch: 1/20... Step: 70... Loss: 3.1069... Val Loss: 3.1103\n",
            "Epoch: 1/20... Step: 80... Loss: 3.1110... Val Loss: 3.0995\n",
            "Epoch: 1/20... Step: 90... Loss: 3.0920... Val Loss: 3.0770\n",
            "Epoch: 1/20... Step: 100... Loss: 3.0413... Val Loss: 3.0204\n",
            "Epoch: 1/20... Step: 110... Loss: 2.9446... Val Loss: 2.9244\n",
            "Epoch: 1/20... Step: 120... Loss: 2.8375... Val Loss: 2.8138\n",
            "Epoch: 1/20... Step: 130... Loss: 2.7398... Val Loss: 2.7006\n",
            "Epoch: 2/20... Step: 140... Loss: 2.6571... Val Loss: 2.6125\n",
            "Epoch: 2/20... Step: 150... Loss: 2.5869... Val Loss: 2.5333\n",
            "Epoch: 2/20... Step: 160... Loss: 2.5227... Val Loss: 2.4792\n",
            "Epoch: 2/20... Step: 170... Loss: 2.4505... Val Loss: 2.4391\n",
            "Epoch: 2/20... Step: 180... Loss: 2.4236... Val Loss: 2.3997\n",
            "Epoch: 2/20... Step: 190... Loss: 2.3758... Val Loss: 2.3716\n",
            "Epoch: 2/20... Step: 200... Loss: 2.3666... Val Loss: 2.3381\n",
            "Epoch: 2/20... Step: 210... Loss: 2.3304... Val Loss: 2.3062\n",
            "Epoch: 2/20... Step: 220... Loss: 2.2892... Val Loss: 2.2767\n",
            "Epoch: 2/20... Step: 230... Loss: 2.2832... Val Loss: 2.2492\n",
            "Epoch: 2/20... Step: 240... Loss: 2.2554... Val Loss: 2.2185\n",
            "Epoch: 2/20... Step: 250... Loss: 2.1901... Val Loss: 2.1935\n",
            "Epoch: 2/20... Step: 260... Loss: 2.1634... Val Loss: 2.1680\n",
            "Epoch: 2/20... Step: 270... Loss: 2.1825... Val Loss: 2.1506\n",
            "Epoch: 3/20... Step: 280... Loss: 2.1763... Val Loss: 2.1240\n",
            "Epoch: 3/20... Step: 290... Loss: 2.1337... Val Loss: 2.0983\n",
            "Epoch: 3/20... Step: 300... Loss: 2.1134... Val Loss: 2.0799\n",
            "Epoch: 3/20... Step: 310... Loss: 2.0787... Val Loss: 2.0596\n",
            "Epoch: 3/20... Step: 320... Loss: 2.0558... Val Loss: 2.0397\n",
            "Epoch: 3/20... Step: 330... Loss: 2.0252... Val Loss: 2.0314\n",
            "Epoch: 3/20... Step: 340... Loss: 2.0540... Val Loss: 2.0088\n",
            "Epoch: 3/20... Step: 350... Loss: 2.0307... Val Loss: 1.9960\n",
            "Epoch: 3/20... Step: 360... Loss: 1.9646... Val Loss: 1.9705\n",
            "Epoch: 3/20... Step: 370... Loss: 1.9882... Val Loss: 1.9559\n",
            "Epoch: 3/20... Step: 380... Loss: 1.9657... Val Loss: 1.9418\n",
            "Epoch: 3/20... Step: 390... Loss: 1.9400... Val Loss: 1.9262\n",
            "Epoch: 3/20... Step: 400... Loss: 1.9065... Val Loss: 1.9125\n",
            "Epoch: 3/20... Step: 410... Loss: 1.9304... Val Loss: 1.8970\n",
            "Epoch: 4/20... Step: 420... Loss: 1.9093... Val Loss: 1.8841\n",
            "Epoch: 4/20... Step: 430... Loss: 1.9064... Val Loss: 1.8711\n",
            "Epoch: 4/20... Step: 440... Loss: 1.8853... Val Loss: 1.8564\n",
            "Epoch: 4/20... Step: 450... Loss: 1.8265... Val Loss: 1.8423\n",
            "Epoch: 4/20... Step: 460... Loss: 1.8173... Val Loss: 1.8344\n",
            "Epoch: 4/20... Step: 470... Loss: 1.8454... Val Loss: 1.8203\n",
            "Epoch: 4/20... Step: 480... Loss: 1.8245... Val Loss: 1.8058\n",
            "Epoch: 4/20... Step: 490... Loss: 1.8366... Val Loss: 1.7990\n",
            "Epoch: 4/20... Step: 500... Loss: 1.8308... Val Loss: 1.7843\n",
            "Epoch: 4/20... Step: 510... Loss: 1.8033... Val Loss: 1.7752\n",
            "Epoch: 4/20... Step: 520... Loss: 1.8184... Val Loss: 1.7657\n",
            "Epoch: 4/20... Step: 530... Loss: 1.7733... Val Loss: 1.7561\n",
            "Epoch: 4/20... Step: 540... Loss: 1.7282... Val Loss: 1.7480\n",
            "Epoch: 4/20... Step: 550... Loss: 1.7808... Val Loss: 1.7329\n",
            "Epoch: 5/20... Step: 560... Loss: 1.7528... Val Loss: 1.7278\n",
            "Epoch: 5/20... Step: 570... Loss: 1.7364... Val Loss: 1.7178\n",
            "Epoch: 5/20... Step: 580... Loss: 1.7241... Val Loss: 1.7074\n",
            "Epoch: 5/20... Step: 590... Loss: 1.7102... Val Loss: 1.6971\n",
            "Epoch: 5/20... Step: 600... Loss: 1.7016... Val Loss: 1.6927\n",
            "Epoch: 5/20... Step: 610... Loss: 1.6840... Val Loss: 1.6873\n",
            "Epoch: 5/20... Step: 620... Loss: 1.6958... Val Loss: 1.6755\n",
            "Epoch: 5/20... Step: 630... Loss: 1.6984... Val Loss: 1.6720\n",
            "Epoch: 5/20... Step: 640... Loss: 1.6732... Val Loss: 1.6642\n",
            "Epoch: 5/20... Step: 650... Loss: 1.6658... Val Loss: 1.6573\n",
            "Epoch: 5/20... Step: 660... Loss: 1.6440... Val Loss: 1.6526\n",
            "Epoch: 5/20... Step: 670... Loss: 1.6721... Val Loss: 1.6462\n",
            "Epoch: 5/20... Step: 680... Loss: 1.6670... Val Loss: 1.6364\n",
            "Epoch: 5/20... Step: 690... Loss: 1.6368... Val Loss: 1.6315\n",
            "Epoch: 6/20... Step: 700... Loss: 1.6417... Val Loss: 1.6263\n",
            "Epoch: 6/20... Step: 710... Loss: 1.6357... Val Loss: 1.6187\n",
            "Epoch: 6/20... Step: 720... Loss: 1.6224... Val Loss: 1.6113\n",
            "Epoch: 6/20... Step: 730... Loss: 1.6307... Val Loss: 1.6056\n",
            "Epoch: 6/20... Step: 740... Loss: 1.5985... Val Loss: 1.6010\n",
            "Epoch: 6/20... Step: 750... Loss: 1.5790... Val Loss: 1.5964\n",
            "Epoch: 6/20... Step: 760... Loss: 1.6211... Val Loss: 1.5920\n",
            "Epoch: 6/20... Step: 770... Loss: 1.5926... Val Loss: 1.5871\n",
            "Epoch: 6/20... Step: 780... Loss: 1.5756... Val Loss: 1.5802\n",
            "Epoch: 6/20... Step: 790... Loss: 1.5706... Val Loss: 1.5777\n",
            "Epoch: 6/20... Step: 800... Loss: 1.5925... Val Loss: 1.5732\n",
            "Epoch: 6/20... Step: 810... Loss: 1.5754... Val Loss: 1.5668\n",
            "Epoch: 6/20... Step: 820... Loss: 1.5306... Val Loss: 1.5644\n",
            "Epoch: 6/20... Step: 830... Loss: 1.5842... Val Loss: 1.5569\n",
            "Epoch: 7/20... Step: 840... Loss: 1.5362... Val Loss: 1.5551\n",
            "Epoch: 7/20... Step: 850... Loss: 1.5585... Val Loss: 1.5494\n",
            "Epoch: 7/20... Step: 860... Loss: 1.5424... Val Loss: 1.5440\n",
            "Epoch: 7/20... Step: 870... Loss: 1.5444... Val Loss: 1.5372\n",
            "Epoch: 7/20... Step: 880... Loss: 1.5446... Val Loss: 1.5366\n",
            "Epoch: 7/20... Step: 890... Loss: 1.5423... Val Loss: 1.5337\n",
            "Epoch: 7/20... Step: 900... Loss: 1.5311... Val Loss: 1.5328\n",
            "Epoch: 7/20... Step: 910... Loss: 1.5074... Val Loss: 1.5241\n",
            "Epoch: 7/20... Step: 920... Loss: 1.5366... Val Loss: 1.5235\n",
            "Epoch: 7/20... Step: 930... Loss: 1.5138... Val Loss: 1.5179\n",
            "Epoch: 7/20... Step: 940... Loss: 1.5186... Val Loss: 1.5178\n",
            "Epoch: 7/20... Step: 950... Loss: 1.5272... Val Loss: 1.5128\n",
            "Epoch: 7/20... Step: 960... Loss: 1.5221... Val Loss: 1.5121\n",
            "Epoch: 7/20... Step: 970... Loss: 1.5234... Val Loss: 1.5060\n",
            "Epoch: 8/20... Step: 980... Loss: 1.4990... Val Loss: 1.5039\n",
            "Epoch: 8/20... Step: 990... Loss: 1.4968... Val Loss: 1.4949\n",
            "Epoch: 8/20... Step: 1000... Loss: 1.4965... Val Loss: 1.4933\n",
            "Epoch: 8/20... Step: 1010... Loss: 1.5248... Val Loss: 1.4910\n",
            "Epoch: 8/20... Step: 1020... Loss: 1.5017... Val Loss: 1.4899\n",
            "Epoch: 8/20... Step: 1030... Loss: 1.4764... Val Loss: 1.4842\n",
            "Epoch: 8/20... Step: 1040... Loss: 1.4872... Val Loss: 1.4844\n",
            "Epoch: 8/20... Step: 1050... Loss: 1.4742... Val Loss: 1.4789\n",
            "Epoch: 8/20... Step: 1060... Loss: 1.4742... Val Loss: 1.4795\n",
            "Epoch: 8/20... Step: 1070... Loss: 1.4851... Val Loss: 1.4753\n",
            "Epoch: 8/20... Step: 1080... Loss: 1.4746... Val Loss: 1.4712\n",
            "Epoch: 8/20... Step: 1090... Loss: 1.4570... Val Loss: 1.4694\n",
            "Epoch: 8/20... Step: 1100... Loss: 1.4529... Val Loss: 1.4665\n",
            "Epoch: 8/20... Step: 1110... Loss: 1.4538... Val Loss: 1.4626\n",
            "Epoch: 9/20... Step: 1120... Loss: 1.4696... Val Loss: 1.4588\n",
            "Epoch: 9/20... Step: 1130... Loss: 1.4590... Val Loss: 1.4559\n",
            "Epoch: 9/20... Step: 1140... Loss: 1.4720... Val Loss: 1.4497\n",
            "Epoch: 9/20... Step: 1150... Loss: 1.4799... Val Loss: 1.4527\n",
            "Epoch: 9/20... Step: 1160... Loss: 1.4285... Val Loss: 1.4522\n",
            "Epoch: 9/20... Step: 1170... Loss: 1.4492... Val Loss: 1.4492\n",
            "Epoch: 9/20... Step: 1180... Loss: 1.4323... Val Loss: 1.4447\n",
            "Epoch: 9/20... Step: 1190... Loss: 1.4623... Val Loss: 1.4459\n",
            "Epoch: 9/20... Step: 1200... Loss: 1.4141... Val Loss: 1.4442\n",
            "Epoch: 9/20... Step: 1210... Loss: 1.4282... Val Loss: 1.4384\n",
            "Epoch: 9/20... Step: 1220... Loss: 1.4341... Val Loss: 1.4366\n",
            "Epoch: 9/20... Step: 1230... Loss: 1.4142... Val Loss: 1.4332\n",
            "Epoch: 9/20... Step: 1240... Loss: 1.4178... Val Loss: 1.4351\n",
            "Epoch: 9/20... Step: 1250... Loss: 1.4331... Val Loss: 1.4280\n",
            "Epoch: 10/20... Step: 1260... Loss: 1.4336... Val Loss: 1.4278\n",
            "Epoch: 10/20... Step: 1270... Loss: 1.4284... Val Loss: 1.4257\n",
            "Epoch: 10/20... Step: 1280... Loss: 1.4319... Val Loss: 1.4231\n",
            "Epoch: 10/20... Step: 1290... Loss: 1.4310... Val Loss: 1.4229\n",
            "Epoch: 10/20... Step: 1300... Loss: 1.4217... Val Loss: 1.4234\n",
            "Epoch: 10/20... Step: 1310... Loss: 1.4234... Val Loss: 1.4180\n",
            "Epoch: 10/20... Step: 1320... Loss: 1.3873... Val Loss: 1.4190\n",
            "Epoch: 10/20... Step: 1330... Loss: 1.3898... Val Loss: 1.4145\n",
            "Epoch: 10/20... Step: 1340... Loss: 1.3936... Val Loss: 1.4117\n",
            "Epoch: 10/20... Step: 1350... Loss: 1.3751... Val Loss: 1.4101\n",
            "Epoch: 10/20... Step: 1360... Loss: 1.3786... Val Loss: 1.4081\n",
            "Epoch: 10/20... Step: 1370... Loss: 1.3791... Val Loss: 1.4076\n",
            "Epoch: 10/20... Step: 1380... Loss: 1.4190... Val Loss: 1.4056\n",
            "Epoch: 10/20... Step: 1390... Loss: 1.4172... Val Loss: 1.4038\n",
            "Epoch: 11/20... Step: 1400... Loss: 1.4187... Val Loss: 1.4025\n",
            "Epoch: 11/20... Step: 1410... Loss: 1.4305... Val Loss: 1.4002\n",
            "Epoch: 11/20... Step: 1420... Loss: 1.4260... Val Loss: 1.3978\n",
            "Epoch: 11/20... Step: 1430... Loss: 1.3850... Val Loss: 1.3980\n",
            "Epoch: 11/20... Step: 1440... Loss: 1.4097... Val Loss: 1.3983\n",
            "Epoch: 11/20... Step: 1450... Loss: 1.3400... Val Loss: 1.3985\n",
            "Epoch: 11/20... Step: 1460... Loss: 1.3620... Val Loss: 1.3916\n",
            "Epoch: 11/20... Step: 1470... Loss: 1.3614... Val Loss: 1.3917\n",
            "Epoch: 11/20... Step: 1480... Loss: 1.3850... Val Loss: 1.3893\n",
            "Epoch: 11/20... Step: 1490... Loss: 1.3666... Val Loss: 1.3859\n",
            "Epoch: 11/20... Step: 1500... Loss: 1.3592... Val Loss: 1.3855\n",
            "Epoch: 11/20... Step: 1510... Loss: 1.3309... Val Loss: 1.3870\n",
            "Epoch: 11/20... Step: 1520... Loss: 1.3734... Val Loss: 1.3846\n",
            "Epoch: 12/20... Step: 1530... Loss: 1.4318... Val Loss: 1.3823\n",
            "Epoch: 12/20... Step: 1540... Loss: 1.3809... Val Loss: 1.3823\n",
            "Epoch: 12/20... Step: 1550... Loss: 1.3849... Val Loss: 1.3801\n",
            "Epoch: 12/20... Step: 1560... Loss: 1.3905... Val Loss: 1.3783\n",
            "Epoch: 12/20... Step: 1570... Loss: 1.3438... Val Loss: 1.3815\n",
            "Epoch: 12/20... Step: 1580... Loss: 1.3232... Val Loss: 1.3827\n",
            "Epoch: 12/20... Step: 1590... Loss: 1.3288... Val Loss: 1.3833\n",
            "Epoch: 12/20... Step: 1600... Loss: 1.3447... Val Loss: 1.3770\n",
            "Epoch: 12/20... Step: 1610... Loss: 1.3432... Val Loss: 1.3769\n",
            "Epoch: 12/20... Step: 1620... Loss: 1.3322... Val Loss: 1.3725\n",
            "Epoch: 12/20... Step: 1630... Loss: 1.3554... Val Loss: 1.3703\n",
            "Epoch: 12/20... Step: 1640... Loss: 1.3382... Val Loss: 1.3718\n",
            "Epoch: 12/20... Step: 1650... Loss: 1.3119... Val Loss: 1.3711\n",
            "Epoch: 12/20... Step: 1660... Loss: 1.3736... Val Loss: 1.3683\n",
            "Epoch: 13/20... Step: 1670... Loss: 1.3375... Val Loss: 1.3677\n",
            "Epoch: 13/20... Step: 1680... Loss: 1.3472... Val Loss: 1.3662\n",
            "Epoch: 13/20... Step: 1690... Loss: 1.3271... Val Loss: 1.3606\n",
            "Epoch: 13/20... Step: 1700... Loss: 1.3310... Val Loss: 1.3636\n",
            "Epoch: 13/20... Step: 1710... Loss: 1.3071... Val Loss: 1.3615\n",
            "Epoch: 13/20... Step: 1720... Loss: 1.3164... Val Loss: 1.3616\n",
            "Epoch: 13/20... Step: 1730... Loss: 1.3460... Val Loss: 1.3606\n",
            "Epoch: 13/20... Step: 1740... Loss: 1.3189... Val Loss: 1.3594\n",
            "Epoch: 13/20... Step: 1750... Loss: 1.2950... Val Loss: 1.3574\n",
            "Epoch: 13/20... Step: 1760... Loss: 1.3185... Val Loss: 1.3570\n",
            "Epoch: 13/20... Step: 1770... Loss: 1.3384... Val Loss: 1.3524\n",
            "Epoch: 13/20... Step: 1780... Loss: 1.3200... Val Loss: 1.3550\n",
            "Epoch: 13/20... Step: 1790... Loss: 1.3014... Val Loss: 1.3562\n",
            "Epoch: 13/20... Step: 1800... Loss: 1.3301... Val Loss: 1.3544\n",
            "Epoch: 14/20... Step: 1810... Loss: 1.3307... Val Loss: 1.3543\n",
            "Epoch: 14/20... Step: 1820... Loss: 1.3200... Val Loss: 1.3474\n",
            "Epoch: 14/20... Step: 1830... Loss: 1.3296... Val Loss: 1.3486\n",
            "Epoch: 14/20... Step: 1840... Loss: 1.2798... Val Loss: 1.3454\n",
            "Epoch: 14/20... Step: 1850... Loss: 1.2601... Val Loss: 1.3506\n",
            "Epoch: 14/20... Step: 1860... Loss: 1.3163... Val Loss: 1.3500\n",
            "Epoch: 14/20... Step: 1870... Loss: 1.3286... Val Loss: 1.3492\n",
            "Epoch: 14/20... Step: 1880... Loss: 1.3191... Val Loss: 1.3483\n",
            "Epoch: 14/20... Step: 1890... Loss: 1.3428... Val Loss: 1.3466\n",
            "Epoch: 14/20... Step: 1900... Loss: 1.3051... Val Loss: 1.3444\n",
            "Epoch: 14/20... Step: 1910... Loss: 1.3176... Val Loss: 1.3416\n",
            "Epoch: 14/20... Step: 1920... Loss: 1.3095... Val Loss: 1.3391\n",
            "Epoch: 14/20... Step: 1930... Loss: 1.2690... Val Loss: 1.3436\n",
            "Epoch: 14/20... Step: 1940... Loss: 1.3326... Val Loss: 1.3417\n",
            "Epoch: 15/20... Step: 1950... Loss: 1.3072... Val Loss: 1.3389\n",
            "Epoch: 15/20... Step: 1960... Loss: 1.3108... Val Loss: 1.3364\n",
            "Epoch: 15/20... Step: 1970... Loss: 1.2879... Val Loss: 1.3373\n",
            "Epoch: 15/20... Step: 1980... Loss: 1.2767... Val Loss: 1.3369\n",
            "Epoch: 15/20... Step: 1990... Loss: 1.2930... Val Loss: 1.3397\n",
            "Epoch: 15/20... Step: 2000... Loss: 1.2791... Val Loss: 1.3368\n",
            "Epoch: 15/20... Step: 2010... Loss: 1.2962... Val Loss: 1.3357\n",
            "Epoch: 15/20... Step: 2020... Loss: 1.3118... Val Loss: 1.3343\n",
            "Epoch: 15/20... Step: 2030... Loss: 1.2751... Val Loss: 1.3344\n",
            "Epoch: 15/20... Step: 2040... Loss: 1.2988... Val Loss: 1.3345\n",
            "Epoch: 15/20... Step: 2050... Loss: 1.2796... Val Loss: 1.3317\n",
            "Epoch: 15/20... Step: 2060... Loss: 1.2920... Val Loss: 1.3277\n",
            "Epoch: 15/20... Step: 2070... Loss: 1.2941... Val Loss: 1.3320\n",
            "Epoch: 15/20... Step: 2080... Loss: 1.2942... Val Loss: 1.3318\n",
            "Epoch: 16/20... Step: 2090... Loss: 1.2982... Val Loss: 1.3292\n",
            "Epoch: 16/20... Step: 2100... Loss: 1.2807... Val Loss: 1.3243\n",
            "Epoch: 16/20... Step: 2110... Loss: 1.2716... Val Loss: 1.3306\n",
            "Epoch: 16/20... Step: 2120... Loss: 1.2872... Val Loss: 1.3263\n",
            "Epoch: 16/20... Step: 2130... Loss: 1.2640... Val Loss: 1.3329\n",
            "Epoch: 16/20... Step: 2140... Loss: 1.2721... Val Loss: 1.3229\n",
            "Epoch: 16/20... Step: 2150... Loss: 1.2970... Val Loss: 1.3252\n",
            "Epoch: 16/20... Step: 2160... Loss: 1.2669... Val Loss: 1.3271\n",
            "Epoch: 16/20... Step: 2170... Loss: 1.2785... Val Loss: 1.3236\n",
            "Epoch: 16/20... Step: 2180... Loss: 1.2670... Val Loss: 1.3221\n",
            "Epoch: 16/20... Step: 2190... Loss: 1.2934... Val Loss: 1.3218\n",
            "Epoch: 16/20... Step: 2200... Loss: 1.2638... Val Loss: 1.3208\n",
            "Epoch: 16/20... Step: 2210... Loss: 1.2251... Val Loss: 1.3238\n",
            "Epoch: 16/20... Step: 2220... Loss: 1.2738... Val Loss: 1.3220\n",
            "Epoch: 17/20... Step: 2230... Loss: 1.2621... Val Loss: 1.3192\n",
            "Epoch: 17/20... Step: 2240... Loss: 1.2660... Val Loss: 1.3173\n",
            "Epoch: 17/20... Step: 2250... Loss: 1.2520... Val Loss: 1.3189\n",
            "Epoch: 17/20... Step: 2260... Loss: 1.2593... Val Loss: 1.3157\n",
            "Epoch: 17/20... Step: 2270... Loss: 1.2654... Val Loss: 1.3204\n",
            "Epoch: 17/20... Step: 2280... Loss: 1.2804... Val Loss: 1.3140\n",
            "Epoch: 17/20... Step: 2290... Loss: 1.2705... Val Loss: 1.3151\n",
            "Epoch: 17/20... Step: 2300... Loss: 1.2476... Val Loss: 1.3149\n",
            "Epoch: 17/20... Step: 2310... Loss: 1.2584... Val Loss: 1.3131\n",
            "Epoch: 17/20... Step: 2320... Loss: 1.2551... Val Loss: 1.3123\n",
            "Epoch: 17/20... Step: 2330... Loss: 1.2575... Val Loss: 1.3130\n",
            "Epoch: 17/20... Step: 2340... Loss: 1.2659... Val Loss: 1.3097\n",
            "Epoch: 17/20... Step: 2350... Loss: 1.2712... Val Loss: 1.3117\n",
            "Epoch: 17/20... Step: 2360... Loss: 1.2746... Val Loss: 1.3142\n",
            "Epoch: 18/20... Step: 2370... Loss: 1.2514... Val Loss: 1.3084\n",
            "Epoch: 18/20... Step: 2380... Loss: 1.2454... Val Loss: 1.3096\n",
            "Epoch: 18/20... Step: 2390... Loss: 1.2478... Val Loss: 1.3098\n",
            "Epoch: 18/20... Step: 2400... Loss: 1.2810... Val Loss: 1.3089\n",
            "Epoch: 18/20... Step: 2410... Loss: 1.2693... Val Loss: 1.3112\n",
            "Epoch: 18/20... Step: 2420... Loss: 1.2517... Val Loss: 1.3031\n",
            "Epoch: 18/20... Step: 2430... Loss: 1.2562... Val Loss: 1.3083\n",
            "Epoch: 18/20... Step: 2440... Loss: 1.2468... Val Loss: 1.3090\n",
            "Epoch: 18/20... Step: 2450... Loss: 1.2321... Val Loss: 1.3055\n",
            "Epoch: 18/20... Step: 2460... Loss: 1.2583... Val Loss: 1.3034\n",
            "Epoch: 18/20... Step: 2470... Loss: 1.2410... Val Loss: 1.3046\n",
            "Epoch: 18/20... Step: 2480... Loss: 1.2420... Val Loss: 1.3027\n",
            "Epoch: 18/20... Step: 2490... Loss: 1.2375... Val Loss: 1.3005\n",
            "Epoch: 18/20... Step: 2500... Loss: 1.2261... Val Loss: 1.3073\n",
            "Epoch: 19/20... Step: 2510... Loss: 1.2433... Val Loss: 1.3001\n",
            "Epoch: 19/20... Step: 2520... Loss: 1.2432... Val Loss: 1.3029\n",
            "Epoch: 19/20... Step: 2530... Loss: 1.2552... Val Loss: 1.3050\n",
            "Epoch: 19/20... Step: 2540... Loss: 1.2695... Val Loss: 1.2995\n",
            "Epoch: 19/20... Step: 2550... Loss: 1.2288... Val Loss: 1.3057\n",
            "Epoch: 19/20... Step: 2560... Loss: 1.2359... Val Loss: 1.3002\n",
            "Epoch: 19/20... Step: 2570... Loss: 1.2241... Val Loss: 1.2996\n",
            "Epoch: 19/20... Step: 2580... Loss: 1.2674... Val Loss: 1.2978\n",
            "Epoch: 19/20... Step: 2590... Loss: 1.2275... Val Loss: 1.2958\n",
            "Epoch: 19/20... Step: 2600... Loss: 1.2293... Val Loss: 1.2964\n",
            "Epoch: 19/20... Step: 2610... Loss: 1.2398... Val Loss: 1.2995\n",
            "Epoch: 19/20... Step: 2620... Loss: 1.2259... Val Loss: 1.2995\n",
            "Epoch: 19/20... Step: 2630... Loss: 1.2330... Val Loss: 1.3008\n",
            "Epoch: 19/20... Step: 2640... Loss: 1.2471... Val Loss: 1.2989\n",
            "Epoch: 20/20... Step: 2650... Loss: 1.2503... Val Loss: 1.2959\n",
            "Epoch: 20/20... Step: 2660... Loss: 1.2402... Val Loss: 1.3003\n",
            "Epoch: 20/20... Step: 2670... Loss: 1.2511... Val Loss: 1.2950\n",
            "Epoch: 20/20... Step: 2680... Loss: 1.2457... Val Loss: 1.2949\n",
            "Epoch: 20/20... Step: 2690... Loss: 1.2390... Val Loss: 1.2979\n",
            "Epoch: 20/20... Step: 2700... Loss: 1.2427... Val Loss: 1.2926\n",
            "Epoch: 20/20... Step: 2710... Loss: 1.2123... Val Loss: 1.2930\n",
            "Epoch: 20/20... Step: 2720... Loss: 1.2158... Val Loss: 1.2941\n",
            "Epoch: 20/20... Step: 2730... Loss: 1.2057... Val Loss: 1.2902\n",
            "Epoch: 20/20... Step: 2740... Loss: 1.2172... Val Loss: 1.2935\n",
            "Epoch: 20/20... Step: 2750... Loss: 1.2151... Val Loss: 1.2925\n",
            "Epoch: 20/20... Step: 2760... Loss: 1.2159... Val Loss: 1.2883\n",
            "Epoch: 20/20... Step: 2770... Loss: 1.2497... Val Loss: 1.2901\n",
            "Epoch: 20/20... Step: 2780... Loss: 1.2672... Val Loss: 1.2927\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SxWxpF4LZlCo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model_name = 'rnn_20_epoch.net'\n",
        "\n",
        "checkpoint = {'n_hidden': net.n_hidden,\n",
        "              'n_layers': net.n_layers,\n",
        "              'state_dict': net.state_dict(),\n",
        "              'tokens': net.chars}\n",
        "\n",
        "with open(model_name, 'wb') as f:\n",
        "    torch.save(checkpoint, f)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S6HzKNvwbbUn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def predict(net, char, h=None, top_k=None):\n",
        "        ''' Given a character, predict the next character.\n",
        "            Returns the predicted character and the hidden state.\n",
        "        '''\n",
        "        \n",
        "        x = np.array([[net.char2int[char]]])\n",
        "        x = one_hot_encode(x, len(net.chars))\n",
        "        inputs = torch.from_numpy(x)\n",
        "        \n",
        "        if(train_on_gpu):\n",
        "            inputs = inputs.cuda()\n",
        "        \n",
        "        h = tuple([each.data for each in h])\n",
        "        out, h = net(inputs, h)\n",
        "\n",
        "        p = F.softmax(out, dim=1).data\n",
        "        if(train_on_gpu):\n",
        "            p = p.cpu() # move to cpu\n",
        "        \n",
        "        if top_k is None:\n",
        "            top_ch = np.arange(len(net.chars))\n",
        "        else:\n",
        "            p, top_ch = p.topk(top_k)\n",
        "            top_ch = top_ch.numpy().squeeze()\n",
        "        \n",
        "        p = p.numpy().squeeze()\n",
        "        char = np.random.choice(top_ch, p=p/p.sum())\n",
        "        \n",
        "        return net.int2char[char], h"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U6cMJrT9gCXv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def sample(net, size, prime='The', top_k=None):\n",
        "        \n",
        "    if(train_on_gpu):\n",
        "        net.cuda()\n",
        "    else:\n",
        "        net.cpu()\n",
        "    \n",
        "    net.eval() # eval mode\n",
        "    \n",
        "    chars = [ch for ch in prime]\n",
        "    h = net.init_hidden(1)\n",
        "    for ch in prime:\n",
        "        char, h = predict(net, ch, h, top_k=top_k)\n",
        "    \n",
        "    chars.append(char)\n",
        "    for ii in range(size):\n",
        "        char, h = predict(net, chars[-1], h, top_k=top_k)\n",
        "        chars.append(char)\n",
        "    print(chars)\n",
        "    return ''.join(chars)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GGOme6xYgFzB",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 309
        },
        "outputId": "bd1dc9e1-8112-46a0-fd47-825fea6a0230"
      },
      "source": [
        "print(sample(net, 1000, prime='Anna', top_k=5))"
      ],
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['A', 'n', 'n', 'a', ',', ' ', 't', 'e', 'l', 'l', 'i', 'n', 'g', ' ', 't', 'h', 'e', 'm', ' ', 't', 'h', 'e', ' ', 'm', 'o', 'n', 'e', 'y', '\\n', 'a', 'n', 'd', ' ', 't', 'h', 'e', ' ', 'f', 'r', 'i', 'g', 'h', 't', 'e', 'n', 'e', 'd', ' ', 'w', 'o', 'r', 'd', 's', ',', ' ', 'a', 'n', 'd', ' ', 's', 't', 'r', 'u', 'c', 'k', ' ', 'h', 'i', 's', ' ', 'f', 'a', 'c', 'e', ',', ' ', 'h', 'e', ' ', 'h', 'a', 'd', ' ', 'b', 'e', 'g', 'i', 'n', 'n', 'e', 'd', ',', ' ', 'a', 'n', 'd', ' ', 't', 'h', 'a', 't', ' ', 'h', 'e', ' ', 'h', 'a', 'd', ' ', 'b', 'e', 'e', 'n', ' ', 't', 'h', 'e', ' ', 's', 'e', 'n', 's', 'e', ' ', 'o', 'f', ' ', 't', 'h', 'e', ' ', 'c', 'o', 'u', 'n', 't', 'r', 'y', \"'\", 's', ',', ' ', 't', 'h', 'e', ' ', 's', 't', 'r', 'i', 'n', 'g', ' ', 'w', 'o', 'u', 'l', 'd', ' ', 'b', 'e', ' ', 't', 'h', 'r', 'o', 'u', 'g', 'h', ' ', 'o', 'n', 'e', '\\n', 's', 'i', 'n', 'c', 'e', ' ', 't', 'h', 'e', 'r', 'e', ' ', 'w', 'o', 'u', 'l', 'd', ' ', 'b', 'e', ' ', 't', 'o', ' ', 'h', 'a', 'v', 'e', ' ', 'a', ' ', 'p', 'e', 'a', 's', 'a', 'n', 't', ' ', 'a', ' ', 'p', 'o', 's', 'i', 't', 'i', 'v', 'e', ' ', 'c', 'o', 'a', 't', ',', ' ', 'w', 'h', 'i', 'c', 'h', ' ', 'i', 't', '\\n', 'w', 'e', 'r', 'e', ',', ' ', 'a', ' ', 's', 'e', 'r', 'e', 'n', 'e', ' ', 'p', 'l', 'a', 'c', 'e', ',', ' ', 'w', 'e', 'r', 'e', '\\n', 's', 'a', 'l', 'i', 'n', 'g', ' ', 'i', 'n', ' ', 'a', ' ', 'l', 'a', 'd', 'i', 'e', 's', '\\n', 'i', 'n', ' ', 'a', ' ', 'p', 'o', 's', 'i', 't', 'i', 'o', 'n', ' ', 'w', 'e', 'r', 'e', ' ', 'a', 'n', 's', 'w', 'i', 'n', 'g', ' ', 't', 'o', ' ', 'h', 'i', 'm', ' ', 'w', 'h', 'e', 'n', ' ', 'h', 'e', ' ', 'h', 'a', 'd', ' ', 'b', 'e', 'e', 'n', ' ', 's', 'u', 'p', 'p', 'o', 'r', 't', 'i', 'n', 'g', ' ', 'i', 't', ' ', 'a', 'n', 'd', ' ', 'a', 's', 'k', 'e', 'd', ' ', 't', 'h', 'e', ' ', 'm', 'a', 'r', 's', 'h', 'a', 'l', ',', ' ', 'a', 'n', 'd', ' ', 't', 'h', 'e', ' ', 'p', 'l', 'e', 'a', 's', 'u', 'r', 'e', 's', ' ', 't', 'h', 'a', 't', ' ', 's', 'h', 'e', ' ', 'w', 'a', 's', ' ', 's', 'u', 'c', 'h', ' ', 't', 'i', 'm', 'e', '.', ' ', 'A', 'n', 'n', 'a', ' ', 's', 'a', 'i', 'd', ' ', 't', 'o', ' ', 'h', 'i', 'm', ' ', 'a', 'n', 'd', ' ', 's', 'a', 'w', ' ', 't', 'h', 'a', 't', ' ', 't', 'h', 'e', ' ', 'p', 'r', 'e', 's', 't', 'i', 'n', 'g', ',', '\\n', 'h', 'e', ' ', 'f', 'r', 'i', 'g', 'h', 't', 'e', 'n', 'e', 'd', ' ', 'h', 'e', 'r', ' ', 'h', 'i', 's', ' ', 's', 'h', 'o', 'o', 't', 'i', 'n', 'g', ' ', 'a', 't', ' ', 't', 'h', 'e', ' ', 'm', 'o', 'm', 'e', 'n', 't', 's', ' ', 'o', 'f', ' ', 'a', 'l', 'l', ' ', 'o', 'f', ' ', 't', 'h', 'e', ' ', 'm', 'i', 'd', 'd', 'l', 'e', ' ', 'o', 'f', ' ', 't', 'h', 'e', ' ', 's', 't', 'r', 'i', 'n', 'g', ' ', 'o', 'f', ' ', 'h', 'i', 's', ' ', 'f', 'a', 'i', 'l', 'e', ' ', 'o', 'u', 't', ' ', 'w', 'i', 't', 'h', ' ', 'a', '\\n', 'l', 'i', 's', 't', 'e', 'n', 'e', 'd', ' ', 'a', 's', ' ', 't', 'h', 'e', ' ', 't', 'h', 'i', 'n', 'g', ' ', 'h', 'u', 'n', 'g', ' ', 'o', 'n', ' ', 't', 'h', 'e', '\\n', 's', 't', 'r', 'i', 'n', 'g', ',', ' ', 's', 'a', 'w', 'i', 'n', 'g', ' ', 't', 'h', 'a', 't', ' ', 'h', 'e', ' ', 'w', 'a', 's', ' ', 'a', 's', ' ', 't', 'h', 'o', 'u', 'g', 'h', ' ', 's', 'h', 'e', ' ', 'h', 'e', 'r', ' ', 'h', 'a', 'n', 'd', ' ', 'o', 'f', ' ', 't', 'h', 'e', ' ', 's', 'a', 'm', 'e', ' ', 't', 'i', 'm', 'e', ' ', 't', 'r', 'i', 'e', 'd', ' ', 't', 'o', ' ', 's', 'e', 'e', ' ', 'w', 'e', 'a', 't', 'i', 'n', 'g', ' ', 'h', 'i', 's', ' ', 'b', 'r', 'o', 't', 'h', 'e', 'r', ',', ' ', 'w', 'h', 'o', ' ', 'h', 'a', 'd', '\\n', 'b', 'e', 'e', 'n', ' ', 'a', 's', ' ', 'h', 'o', 'w', ' ', 's', 'h', 'e', ' ', 'h', 'a', 'd', ' ', 'b', 'e', 'g', 'u', 'n', ',', ' ', 'a', 'n', 'd', ' ', 'h', 'e', '\\n', 's', 'a', 'w', ' ', 't', 'h', 'a', 't', ' ', 't', 'h', 'e', ' ', 's', 'e', 't', 't', 'e', 'r', ' ', 'a', 's', 's', 'o', 'm', 'i', 'n', 'g', ' ', 't', 'h', 'e', ' ', 'c', 'a', 'r', 'e', ' ', 'i', 'n', ' ', 't', 'h', 'e', ' ', 'c', 'o', 'n', 't', 'r', 'a', 'r', 'y', ' ', 'w', 'a', 's', ' ', 'a', 't', ' ', 'o', 'n', 'c', 'e', ' ', 't', 'r', 'u', 'e', ' ', 't', 'h', 'a', 't', ' ', 't', 'h', 'e', 'r', 'e', ' ', 'w', 'i', 's', 'h', 'e', 'd', ' ', 't', 'o', '\\n', 't', 'h', 'e', ' ', 's', 'e', 'r', 'v', 'a', 'n', 't', 's', ' ', 'w', 'o', 'u', 'l', 'd', ' ', 'n', 'o', 't', ' ', 'b', 'e', ' ', 'a', 's', ' ', 'i', 't', ' ', 's', 'e', 'e', 'm', 'e', 'd', ',', ' ', 'a', 'n', 'd', ' ', 't', 'h', 'a', 't', ' ', 'h', 'e', 'r', '\\n', 'e', 'y', 'e', 's', ' ', 'w', 'e', 'r', 'e', ' ', 'a', 'l', 'l', ' ', 't', 'h', 'e', ' ', 's', 't', 'i', 'f', 'f', 'i', 'n', 'g', ' ', 't', 'h', 'e', ' ', 'm', 'o', 'r', 'e', ' ', 'a', 'n', 'd', ' ', 'm', 'o', 'n', 'e', 'y', '\\n', 'o', 'n', ' ', 'h', 'i', 's', ' ', 'f', 'a', 'i', 'l', 'e', 'd', ' ', 'a', 'n', 'd', ' ', 'a', ' ', 'b', 'r', 'i', 'd', 'e', ' ', 'a', 'n', 'd', ' ', 'a', ' ', 's', 'e', 'r', 'f', 'i', 'c', 'e', ' ', 'o', 'f', ' ', 'h', 'i', 's', ' ', 'w', 'o', 'r', 'd', 's', ' ', 'i', 'n', ' ', 't', 'h', 'e', '\\n', 's', 'u', 'n', 's', 'h', 'i', 'n', 'e', 's', 't', '.', ' ', 'H']\n",
            "Anna, telling them the money\n",
            "and the frightened words, and struck his face, he had beginned, and that he had been the sense of the country's, the string would be through one\n",
            "since there would be to have a peasant a positive coat, which it\n",
            "were, a serene place, were\n",
            "saling in a ladies\n",
            "in a position were answing to him when he had been supporting it and asked the marshal, and the pleasures that she was such time. Anna said to him and saw that the presting,\n",
            "he frightened her his shooting at the moments of all of the middle of the string of his faile out with a\n",
            "listened as the thing hung on the\n",
            "string, sawing that he was as though she her hand of the same time tried to see weating his brother, who had\n",
            "been as how she had begun, and he\n",
            "saw that the setter assoming the care in the contrary was at once true that there wished to\n",
            "the servants would not be as it seemed, and that her\n",
            "eyes were all the stiffing the more and money\n",
            "on his failed and a bride and a serfice of his words in the\n",
            "sunshinest. H\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sqtnaHnEgHc3",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "cb228230-dce7-4dbe-da22-d958b6e9acfd"
      },
      "source": [
        "with open('rnn_20_epoch.net', 'rb') as f:\n",
        "    checkpoint = torch.load(f)\n",
        "    \n",
        "loaded = model(checkpoint['tokens'], n_hidden=checkpoint['n_hidden'], n_layers=checkpoint['n_layers'])\n",
        "loaded.load_state_dict(checkpoint['state_dict'])"
      ],
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "IncompatibleKeys(missing_keys=[], unexpected_keys=[])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 36
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IxQEslhIgJPm",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 751
        },
        "outputId": "97b854ff-dac7-43d3-8596-287a7f18d74e"
      },
      "source": [
        "print(sample(loaded, 2000, top_k=5, prime=\"We're in the endgame now\"))"
      ],
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['W', 'e', \"'\", 'r', 'e', ' ', 'i', 'n', ' ', 't', 'h', 'e', ' ', 'e', 'n', 'd', 'g', 'a', 'm', 'e', ' ', 'n', 'o', 'w', ',', '\"', ' ', 's', 'h', 'e', ' ', 's', 'a', 'i', 'd', ',', ' ', 'w', 'a', 's', ' ', 't', 'a', 'l', 'k', 'i', 'n', 'g', ' ', 'a', 'b', 'o', 'u', 't', ' ', 'h', 'i', 'm', '.', '\\n', '\\n', '\"', 'Y', 'o', 'u', '\\n', 'c', 'a', 'n', 'n', 'o', 't', ' ', 'l', 'i', 'k', 'e', ' ', 'm', 'y', ' ', 'w', 'i', 'f', 'e', ' ', 'i', 's', ' ', 's', 'o', ' ', 't', 'a', 'k', 'e', 'n', ' ', 'u', 'p', ' ', 't', 'h', 'e', ' ', 'c', 'o', 'u', 'n', 't', 'r', 'y', ',', ' ', 'a', 'n', 'd', ' ', 't', 'h', 'i', 's', ' ', 's', 'a', 'y', 'i', 'n', 'g', ' ', 'i', 's', ' ', 'a', ' ', 'l', 'o', 'v', 'e', '.', '\\n', '\\n', 'A', ' ', 'm', 'a', 'n', ' ', 'o', 'f', ' ', 't', 'h', 'e', ' ', 'p', 'i', 's', 't', ' ', 'h', 'e', ' ', 'h', 'a', 'd', ' ', 'n', 'o', 't', ' ', 'b', 'e', 'e', 'n', ' ', 't', 'o', ' ', 's', 'p', 'a', 'r', 'k', ' ', 'o', 'f', ' ', 'm', 'y', ' ', 's', 'o', 'r', 'r', 'o', 'w', ',', ' ', 'a', 'n', 'd', ' ', 't', 'h', 'a', 't', ' ', 'y', 'o', 'u', ' ', 'd', 'e', 's', 'c', 'i', 'r', 'e', 'd', ' ', 'h', 'e', 'r', ' ', 'h', 'a', 'n', 'd', 's', '\\n', 'a', 'n', 'd', ' ', 't', 'h', 'o', 'u', 'g', 'h', 't', ' ', 'a', 'n', 'd', ' ', 'm', 'e', 'r', 'e', 's', ' ', 'i', 'n', ' ', 't', 'h', 'i', 's', ' ', 's', 't', 'r', 'i', 'n', 'g', 's', '.', '\\n', 'W', 'a', 's', ' ', 'a', 'l', 'l', ' ', 't', 'h', 'e', ' ', 'c', 'o', 'u', 'n', 't', 'r', 'y', ' ', 'o', 'n', 'l', 'y', ' ', 'i', 'n', ' ', 't', 'h', 'e', ' ', 'd', 'e', 'a', 't', 'h', ' ', 'o', 'f', ' ', 'a', ' ', 'm', 'o', 'n', 'e', 'y', ' ', 'o', 'f', ' ', 'i', 'n', 's', 't', 'a', 'n', 't', 's', ',', ' ', 't', 'h', 'e', ' ', 'f', 'o', 'o', 't', 'm', 'a', 'n', ' ', 'a', 't', ' ', 'h', 'i', 'm', 's', 's', 'e', 's', 's', ' ', 'w', 'i', 't', 'h', ' ', 'h', 'e', 'r', ' ', 't', 'r', 'u', 't', 'h', ' ', 'o', 'f', ' ', 'a', ' ', 'm', 'a', 'n', ' ', 'w', 'o', 'u', 'l', 'd', ' ', 'h', 'a', 'v', 'e', ' ', 'b', 'e', 'e', 'n', ' ', 'i', 'n', ' ', 't', 'h', 'e', ' ', 's', 'a', 'm', 'e', '\\n', 'c', 'a', 'p', 'i', 'o', 'n', ' ', 'o', 'n', ' ', 't', 'h', 'a', 't', ' ', 'm', 'a', 'r', 'r', 'i', 'a', 'g', 'e', ',', ' ', 'h', 'e', ' ', 'd', 'o', 'e', 's', ' ', 'n', 'o', 't', ' ', 'b', 'e', ' ', 'a', 'n', ' ', 'o', 'n', 'e', ' ', 'o', 'f', ' ', 'a', ' ', 'l', 'a', 'd', 'g', 'e', ',', '\\n', 'a', 'n', 'd', ' ', 't', 'h', 'a', 't', ' ', 's', 'h', 'e', ' ', 'w', 'a', 's', ' ', 'a', 'l', 'l', ' ', 's', 'o', 'm', 'e', 't', 'h', 'i', 'n', 'g', '.', ' ', 'S', 'h', 'a', 't', ' ', 'h', 'i', 's', ' ', 'w', 'i', 'f', 'e', ' ', 't', 'h', 'a', 'n', ' ', 't', 'h', 'e', '\\n', 'd', 'o', 'c', 't', 'o', 'r', ',', ' ', 's', 'i', 'g', 'h', 't', ' ', 'o', 'f', ' ', 'a', ' ', 'c', 'h', 'o', 'p', 'e', 'd', ' ', 'o', 'v', 'e', 'r', '\\n', 'h', 'i', 's', ' ', 's', 't', 'e', 'p', 's', ';', ' ', 'h', 'e', ' ', 's', 'a', 'w', ' ', 't', 'h', 'a', 't', ' ', 'i', 't', ' ', 'c', 'a', 'm', 'e', ' ', 'o', 'u', 't', ' ', 'a', 'n', 'd', ' ', 's', 'a', 'w', '\\n', 'h', 'i', 's', ' ', 'b', 'e', 's', 't', ' ', 't', 'h', 'a', 't', ' ', 'h', 'e', ' ', 'h', 'a', 'd', ' ', 'b', 'e', 'e', 'n', ' ', 't', 'h', 'r', 'o', 'u', 'g', 'h', ' ', 't', 'h', 'e', '\\n', 'c', 'o', 'u', 'n', 't', 'r', 'y', ',', ' ', 'h', 'i', 's', ' ', 's', 'e', 'r', 'v', 'i', 'c', 'e', ',', ' ', 'a', 'n', 'd', ' ', 'h', 'e', ' ', 'f', 'o', 'u', 'n', 'd', ' ', 't', 'h', 'a', 't', ' ', 't', 'h', 'e', ' ', 'c', 'h', 'i', 'l', 'd', 'r', 'e', 'n', ' ', 's', 'h', 'o', 'u', 'l', 'd', ' ', 'b', 'e', ' ', 't', 'h', 'e', '\\n', 'c', 'o', 'n', 's', 'i', 'd', 'e', 'r', 'i', 'n', 'g', ' ', 't', 'r', 'e', 'e', 's', ' ', 'w', 'i', 't', 'h', ' ', 't', 'h', 'e', ' ', 's', 'o', 'u', 'n', 'd', ' ', 'o', 'f', ' ', 't', 'h', 'e', ' ', 'p', 'r', 'o', 'f', 'i', 't', ' ', 'o', 'f', ' ', 't', 'h', 'e', ' ', 'c', 'h', 'a', 'i', 'r', ',', ' ', 'a', 'n', 'd', ' ', 's', 'h', 'a', 'm', 'i', 'n', 'g', ' ', 'a', 't', ' ', 't', 'h', 'e', ' ', 'c', 'r', 'o', 'w', 'd', 's', ',', ' ', 't', 'a', 'k', 'i', 'n', 'g', ' ', 'h', 'i', 's', ' ', 's', 'h', 'o', 'u', 'l', 'd', 'e', 'r', 's', ' ', 't', 'o', ' ', 'h', 'i', 's', ' ', 's', 'h', 'o', 'r', 't', ' ', 'f', 'a', 'c', 'e', '.', '\\n', '\\n', '\"', 'I', ' ', 's', 'h', 'a', 'l', 'l', ' ', 's', 'a', 'y', ',', ' ', 'a', 'n', 'd', ' ', 'I', \"'\", 'l', 'l', ' ', 'n', 'e', 'v', 'e', 'r', ' ', 't', 'e', 'l', 'l', '.', ' ', 'A', 'n', 'd', ' ', 'I', ' ', 'w', 'a', 'n', 't', ' ', 't', 'o', ' ', 'b', 'l', 'a', 't', 'e', ' ', 'i', 't', ' ', 'o', 'v', 'i', 'n', 'g', ' ', 't', 'h', 'a', 't', ' ', 'I', ' ', 'c', 'o', 'm', 'e', ',', '\"', ' ', 's', 'a', 'i', 'd', ' ', 't', 'h', 'e', ' ', 'p', 'e', 'o', 'p', 'l', 'e', '.', '\\n', '\\n', '\"', 'I', ' ', 's', 'h', 'a', 'l', 'l', ' ', 'b', 'e', '\\n', 'a', 't', 't', 'a', 't', 'h', 'e', 'd', ' ', 't', 'h', 'a', 't', ' ', 't', 'h', 'e', ' ', 's', 'a', 't', 'i', 's', 'f', 'a', 'c', 't', 'i', 'o', 'n', ' ', 'i', 's', ' ', 's', 'i', 'l', 'l', 'i', 'f', 'i', 'c', 'a', 'r', 'e', ' ', 'a', 't', ' ', 't', 'h', 'e', ' ', 'm', 'a', 'r', 's', 'h', 'a', 'l', ',', '\"', ' ', 's', 'h', 'e', ' ', 's', 'a', 'i', 'd', '.', '\\n', '\\n', '\"', 'W', 'h', 'i', 't', 'e', ' ', 'd', 'i', 's', 'a', 'g', 'r', 'a', 'c', 'u', 't', 'u', 'r', 'e', ' ', 't', 'h', 'a', 't', ' ', 't', 'h', 'e', ' ', 's', 'a', 'm', 'e', ' ', 'a', 'n', 's', 'w', 'e', 'r', ',', ' ', 'y', 'o', 'u', 'r', ' ', 'l', 'a', 's', 't', ' ', 'w', 'a', 's', ' ', 't', 'h', 'e', ' ', 'p', 'r', 'i', 'n', 'c', 'e', 's', 's', ',', ' ', 't', 'o', 'l', 'd', ' ', 'm', 'e', ',', ' ', 'I', ' ', 's', 'h', 'a', 'l', 'l', ' ', 'g', 'e', 't', ' ', 't', 'h', 'e', 'i', 'r', ' ', 'p', 'o', 's', 's', 'e', 's', 's', 'i', 'o', 'n', ' ', 'o', 'f', ' ', 't', 'h', 'e', ' ', 's', 'a', 'm', 'e', ' ', 'w', 'o', 'm', 'e', 'n', '.', ' ', 'T', 'h', 'e', ' ', 'd', 'o', 'c', 't', 'o', 'r', ' ', 't', 'h', 'e', 'n', '\\n', 'h', 'a', 'v', 'e', ' ', 'n', 'o', 't', 'h', 'i', 'n', 'g', ' ', 'a', 'n', 'd', ' ', 's', 'a', 'y', ' ', 't', 'h', 'a', 't', '\\n', 'I', ' ', 'h', 'a', 'v', 'e', '\\n', 'n', 'o', 't', ' ', 's', 'o', 'm', 'e', 't', 'h', 'i', 'n', 'g', ' ', 'b', 'y', ' ', 'm', 'y', ' ', 's', 'o', 'n', ' ', 'o', 'f', ' ', 'h', 'a', 'v', 'i', 'n', 'g', ' ', 'b', 'e', 'e', 'n', '\\n', 't', 'r', 'e', 'a', 't', 'e', 'd', '.', ' ', 'T', 'h', 'e', ' ', 's', 'e', 'c', 'r', 'e', 't', ' ', 'p', 'e', 'a', 's', 'a', 'n', 't', 's', ',', ' ', 't', 'h', 'e', ' ', 'p', 'r', 'i', 'e', 's', 't', ' ', 'i', 'n', '\\n', 't', 'h', 'e', ' ', 'p', 'o', 's', 'i', 't', 'i', 'o', 'n', ' ', 'o', 'f', ' ', 'i', 'm', 'p', 'o', 'r', 't', 'a', 'n', 'c', 'e', ',', '\"', ' ', 'h', 'e', ' ', 's', 'a', 'i', 'd', ',', ' ', 'b', 'u', 't', ' ', 't', 'h', 'i', 's', ' ', 's', 'h', 'a', 'r', 'e', ' ', 's', 'h', 'e', ' ', 'h', 'a', 'd', ' ', 't', 'o', 'o', ' ', 'a', 'r', 'e', 's', 't', ' ', 'b', 'r', 'o', 'u', 'g', 'h', 't', ' ', 't', 'h', 'e', ' ', 's', 'e', 'r', 'v', 'a', 'n', 't', ' ', 'o', 'f', ' ', 't', 'h', 'e', ' ', 's', 'o', 'u', 'l', '.', ' ', 'S', 'h', 'e', ' ', 's', 'a', 'i', 'd', ' ', 't', 'o', ' ', 'h', 'i', 'm', 's', 'e', 'l', 'f', ' ', 'i', 'n', ' ', 't', 'h', 'e', ' ', 's', 'a', 'm', 'e', ' ', 'c', 'o', 'n', 'v', 'o', 'r', 'c', 'a', 't', 'i', 'o', 'n', '.', '\\n', '\\n', '\"', 'I', 't', \"'\", 's', ' ', 'n', 'o', 't', ' ', 't', 'h', 'e', ' ', 'c', 'o', 'm', 'm', 'o', 'n', ' ', 'c', 'o', 'u', 'l', 'd', ' ', 'n', 'o', 't', ' ', 'g', 'r', 'e', 'a', 't', ' ', 't', 'h', 'e', ' ', 'c', 'o', 'u', 'n', 't', 'r', 'y', ',', '\"', ' ', 's', 'h', 'e', ' ', 's', 'a', 'i', 'd', ',', ' ', 'l', 'a', 'i', 'd', ' ', 'h', 'i', 's', ' ', 'h', 'o', 'u', 's', 'e', ',', ' ', 'a', 's', ' ', 'h', 'e', ' ', 's', 'u', 'p', 'p', 'i', 's', 'e', 'd', ' ', 't', 'o', ' ', 'h', 'i', 's', ' ', 'f', 'i', 'g', 'u', 'r', 'e', ' ', 't', 'o', ' ', 't', 'h', 'e', ' ', 's', 't', 'i', 'l', 'l', ' ', 'p', 'o', 's', 'i', 't', 'i', 'o', 'n', ' ', 'o', 'f', ' ', 'h', 'i', 's', ' ', 's', 'i', 's', 't', 'e', 'r', ',', ' ', 't', 'h', 'i', 'n', 'k', 'i', 'n', 'g', ' ', 'a', 'n', 'd', ' ', 's', 't', 'i', 'l', 'l', ' ', 'm', 'o', 'r', 'e', ' ', 'a', 'l', 'l', 'o', 'w', 'e', 'd', ' ', 't', 'h', 'e', ' ', 'b', 'e', 'a', 'u', 't', 'y', ',', ' ', 'a', 'n', 'd', ',', ' ', 't', 'a', 'k', 'i', 'n', 'g', ' ', 'i', 't', ' ', 'o', 'f', 'f', ' ', 'h', 'e', 'r', ' ', 'a', 'n', 'd', ' ', 'w', 'i', 't', 'h', ' ', 't', 'h', 'e', ' ', 'c', 'o', 'u', 'n', 't', 'r', 'y', '.', '\\n', '\\n', '\"', 'I', ' ', 'c', 'a', 'n', \"'\", 't', ' ', 's', 'e', 'e', ' ', 'y', 'o', 'u', 'r', ' ', 'm', 'a', 'r', 'r', 'i', 'i', 'n', ',', ' ', 'w', 'h', 'a', 't', ' ', 'i', 's', ' ', 'i', 'm', ' ', 'I', ' ', 'd', 'o', 'n', \"'\", 't', ' ', 'k', 'n', 'o', 'w', ',', ' ', 'a', 'l', 'l', '\\n', 'y', 'o', 'u', ' ', 'a', 'r', 'e', ' ', 's', 'e', 'n', 'd', ' ', 'a', 'n', 'g', ' ', 't', 'h', 'e', ' ', 's', 'a', 'm', 'e', '?', ' ', 'I', ' ', 's', 'h', 'a', 'l', 'l', ' ', 's', 'a', 'y', ' ', 't', 'o', ' ', 'm', 'e', '.', ' ', 'W', 'h', 'e', 'l', 'e', ' ', 'h', 'e', \"'\", 's', ' ', 's', 'o', ' ', 'm', 'u', 'c', 'h', ' ', 'i', 'n', ' ', 't', 'h', 'e', ' ', 'd', 'r', 'a', 'w', 'i', 'n', 'g', ' ', 'r', 'o', 'o', 'm', ',', '\"', ' ', 'a', 'n', 's', 'w', 'e', 'r', 'e', 'd', ' ', 'V', 'a', 'r', 'e', 'n', 'k', 'a', '.', ' ', '\"', 'I', \"'\", 'm', ' ', 'n', 'o', 't', '\\n', 't', 'e', 'l', 'l', 'i', 'n', 'g', ' ', 'h', 'i', 'm', '.', ' ', 'A', 'n', 'd', ' ', 'i', 't', \"'\", 's', ' ', 'n', 'o', 't', ' ', 'a', ' ', 'c', 'o', 'n', 'v', 'e', 'r', 's', 'a', 't', 'i', 'o', 'n', ' ', 'o', 'f', ' ', 'a', 'n', 'd', ' ', 't', 'h', 'e', ' ', 'p', 'a', 'r', 't', 'y', ' ', 'o', 'f', ' ', 't', 'h', 'e', ' ', 'c', 'o', 'u', 'n', 't', 'r', 'y', '.', '\"', '\\n', '\\n', '\"', 'O', 'n', 'e', ' ', 's', 'a', 'y', 'i', 'n', 'g', '.', '\\n', '\\n', 'I', ' ', 's', 'a', 'w', ' ', 't', 'h', 'a', 't', ' ', 'i', 't', ' ', 'w', 'a', 's', ' ', 'a', ' ', 's', 't', 'r', 'a', 'c', 't', ' ', 'w', 'e', 'l', 'l', '-', '-', 't', 'h', 'a', 't', '\\n', 'h', 'a', 's', ' ', 'a', 'l', 'l', 'o', 'w', ' ', 't', 'h', 'a', 't', ' ', 'i', 'n', ' ', 'h', 'i', 's', ' ', 'b', 'e', 'e', 'n', '\\n', 't', 'e', 'a', 's', 's', ' ', 't', 'o', '\\n', 'b', 'u', 't', ' ', 'h', 'e', 'r', ' ', 't', 'o', ' ', 's', 'e', 'e', ',', ' ', 'a', 'n', 'd', ' ', 'I', '\\n', 'h', 'a', 'v', 'e', ' ', 'n', 'o', 't', 'h', 'i', 'n', 'g', ' ']\n",
            "We're in the endgame now,\" she said, was talking about him.\n",
            "\n",
            "\"You\n",
            "cannot like my wife is so taken up the country, and this saying is a love.\n",
            "\n",
            "A man of the pist he had not been to spark of my sorrow, and that you descired her hands\n",
            "and thought and meres in this strings.\n",
            "Was all the country only in the death of a money of instants, the footman at himssess with her truth of a man would have been in the same\n",
            "capion on that marriage, he does not be an one of a ladge,\n",
            "and that she was all something. Shat his wife than the\n",
            "doctor, sight of a choped over\n",
            "his steps; he saw that it came out and saw\n",
            "his best that he had been through the\n",
            "country, his service, and he found that the children should be the\n",
            "considering trees with the sound of the profit of the chair, and shaming at the crowds, taking his shoulders to his short face.\n",
            "\n",
            "\"I shall say, and I'll never tell. And I want to blate it oving that I come,\" said the people.\n",
            "\n",
            "\"I shall be\n",
            "attathed that the satisfaction is sillificare at the marshal,\" she said.\n",
            "\n",
            "\"White disagracuture that the same answer, your last was the princess, told me, I shall get their possession of the same women. The doctor then\n",
            "have nothing and say that\n",
            "I have\n",
            "not something by my son of having been\n",
            "treated. The secret peasants, the priest in\n",
            "the position of importance,\" he said, but this share she had too arest brought the servant of the soul. She said to himself in the same convorcation.\n",
            "\n",
            "\"It's not the common could not great the country,\" she said, laid his house, as he suppised to his figure to the still position of his sister, thinking and still more allowed the beauty, and, taking it off her and with the country.\n",
            "\n",
            "\"I can't see your marriin, what is im I don't know, all\n",
            "you are send ang the same? I shall say to me. Whele he's so much in the drawing room,\" answered Varenka. \"I'm not\n",
            "telling him. And it's not a conversation of and the party of the country.\"\n",
            "\n",
            "\"One saying.\n",
            "\n",
            "I saw that it was a stract well--that\n",
            "has allow that in his been\n",
            "teass to\n",
            "but her to see, and I\n",
            "have nothing \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TU5u8Dlvj259",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}